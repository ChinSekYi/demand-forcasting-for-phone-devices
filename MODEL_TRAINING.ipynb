{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- Evaluation Metrics Explanation\n",
    "- Modelling\n",
    "- Comparison of Models\n",
    "- Results\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Explanation\n",
    "\n",
    "#### Accuracy\n",
    "Accuracy provides a ratio of correctly predicted observations to the total observations. \n",
    "**Formula**:\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
    "\n",
    "#### Confusion Matrix and Related Terms\n",
    "The confusion matrix is a table layout that allows visualization of the performance of the algorithm, where each number in the matrix represents:\n",
    "- **TP (True Positives)**: Correctly predicted positive observations.\n",
    "- **TN (True Negatives)**: Correctly predicted negative observations.\n",
    "- **FP (False Positives)**: Incorrectly predicted as positive.\n",
    "- **FN (False Negatives)**: Incorrectly predicted as negative.\n",
    "\n",
    "#### Precision, Recall, and F1-Score\n",
    "- **Precision**:\n",
    "$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n",
    "Precision measures the accuracy of positive predictions.\n",
    "\n",
    "- **Recall** (or Sensitivity or TPR):\n",
    "$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n",
    "Recall measures the ability of a model to find all the relevant cases (all positive samples).\n",
    "\n",
    "- **F1-Score**:\n",
    "$$ \\text{F1-Score} = 2 \\times \\left( \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\right) $$\n",
    "The F1-Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. A high F1-score shows a model can classify the positive class correctly, while not misclassifying many negative classes as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/cleaned_df_v2.csv\", index_col=False)\n",
    "df = df.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[df.columns.drop('closing_subs_monthly')]\n",
    "y = df['closing_subs_monthly']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=3244\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
