{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Below are the key classification metrics that we will use to evaluate our model performance:\n",
    "\n",
    "#### Accuracy\n",
    "Accuracy provides a ratio of correctly predicted observations to the total observations. \n",
    "**Formula**:\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
    "\n",
    "#### Confusion Matrix and Related Terms\n",
    "The confusion matrix is a table layout that allows visualization of the performance of the algorithm, where each number in the matrix represents:\n",
    "- **TP (True Positives)**: Correctly predicted positive observations.\n",
    "- **TN (True Negatives)**: Correctly predicted negative observations.\n",
    "- **FP (False Positives)**: Incorrectly predicted as positive.\n",
    "- **FN (False Negatives)**: Incorrectly predicted as negative.\n",
    "\n",
    "#### Precision, Recall, and F1-Score\n",
    "- **Precision**:\n",
    "$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n",
    "Precision measures the accuracy of positive predictions.\n",
    "\n",
    "- **Recall** (or Sensitivity or TPR):\n",
    "$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n",
    "Recall measures the ability of a model to find all the relevant cases (all positive samples).\n",
    "\n",
    "- **F1-Score**:\n",
    "$$ \\text{F1-Score} = 2 \\times \\left( \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\right) $$\n",
    "The F1-Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. A high F1-score shows a model can classify the positive class correctly, while not misclassifying many negative classes as positive."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
